{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dt-cs/Qwen_SFT/blob/main/Qwen3_14b_with_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iH95w8FXt9_O"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install fsspec==2023.9.2\n",
        "    !pip install -U transformers trl\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "#dataset = load_dataset(\"deebak14/rhinoscript_ft_data_04\", split = \"train\")\n",
        "\n",
        "train_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_04\", split = \"train\")\n",
        "eval_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_04_test\", split = \"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen3-14B\"            #\"Qwen/Qwen2.5-Coder-7B-Instruct\" #\"mistralai/Mistral-7B-Instruct-v0.3\" #\"google/gemma-3-12b-it\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Setup bitsandbytes 4-bit config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load the model in 4bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",         # auto-assigns model to GPU if available\n",
        "    torch_dtype=torch.float16, # or 'auto' if you want auto-detection\n",
        ")\n",
        "\n",
        "# Set max sequence length when tokenizing/generating, not in .from_pretrained\n",
        "max_seq_length = 4096\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "c4d88153-fa45-4fea-8817-a858251b429a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 16,056,320 || all params: 14,784,363,520 || trainable%: 0.1086\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=4,                     # LoRA rank\n",
        "    lora_alpha=8,           # LoRA alpha (often 2x r)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",    # Important for language modeling\n",
        ")\n",
        "\n",
        "# Attach LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# (Optional) Print trainable parameters to confirm LoRA is applied\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/qwen_chat_template.jinja\") as f:\n",
        "    tokenizer.chat_template = f.read()"
      ],
      "metadata": {
        "id": "wKrIXeN3g6cP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import apply_chat_template\n",
        "\n",
        "# Pick a few examples from your dataset\n",
        "for i in range(3):\n",
        "    sample = train_dataset[i]\n",
        "    # Apply the chat template with tokenization and assistant mask\n",
        "    output = tokenizer.apply_chat_template(\n",
        "        sample['messages'],\n",
        "        tokenize=True,\n",
        "        return_assistant_tokens_mask=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(\"\".join(map(str, output[\"assistant_masks\"])))\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "OEJy1T5Xl_aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    dataset_text_field=\"messages\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.05,                # Increased for more stable start\n",
        "    num_train_epochs=10,\n",
        "    learning_rate= 2e-6,             #1e-5             # Lowered for better generalization\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.05,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=3407,\n",
        "    report_to=\"none\",\n",
        "    eval_strategy=\"epoch\",          # Evaluate and save every epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,             # Only keep last 3 checkpoints\n",
        "    max_grad_norm=1.0,              # Clip gradients\n",
        "    fp16=True,                      # Enable mixed-precision if available (optional, can remove if not supported)\n",
        "    push_to_hub=False,\n",
        "    neftune_noise_alpha=5,\n",
        "    assistant_only_loss=True,\n",
        "    chat_template_path=\"/content/qwen_chat_template.jinja\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the full input_ids and assistant_masks for a sample\n",
        "sample = trainer.train_dataset[100]\n",
        "input_ids = sample[\"input_ids\"]\n",
        "assistant_masks = sample[\"assistant_masks\"]\n",
        "\n",
        "# Extract token ids for assistant tokens only\n",
        "assistant_token_ids = [tid for tid, mask in zip(input_ids, assistant_masks) if mask == 1]\n",
        "\n",
        "# Now decode ONLY those assistant tokens\n",
        "assistant_text = tokenizer.decode(assistant_token_ids)\n",
        "print(\"Assistant text only:\\n\", assistant_text)\n"
      ],
      "metadata": {
        "id": "EuMl3P5OexGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "6261018f-64af-41bf-e21e-6229d37de009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "13.855 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "348a3a0b-8335-45f8-d424-43306227b89e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1450' max='1450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1450/1450 1:04:31, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.357200</td>\n",
              "      <td>2.056313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.631800</td>\n",
              "      <td>1.410493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.188800</td>\n",
              "      <td>0.995862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.033100</td>\n",
              "      <td>0.898313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.884500</td>\n",
              "      <td>0.877809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.845500</td>\n",
              "      <td>0.871156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.863700</td>\n",
              "      <td>0.867139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.855400</td>\n",
              "      <td>0.865297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.853500</td>\n",
              "      <td>0.864410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.807400</td>\n",
              "      <td>0.864205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "cf2823c4-a07f-440e-aea9-60ac9fb32a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3875.2585 seconds used for training.\n",
            "64.59 minutes used for training.\n",
            "Peak reserved memory = 32.387 GB.\n",
            "Peak reserved memory for training = 18.532 GB.\n",
            "Peak reserved memory % of max memory = 81.874 %.\n",
            "Peak reserved memory for training % of max memory = 46.849 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
        "\n",
        "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before SFT"
      ],
      "metadata": {
        "id": "KwE7s5YtCyOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use a pipeline for easy inference\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\" : \"system\",\n",
        "     \"content\" : \"\"\"You are an expert in Python and Rhino 3D modeling using the rhinoscriptsyntax module and, if required, other native Python modules to accomplish the task. Your task is to interpret, analyze, and understand the user’s query, then respond with a corresponding Python script. Whenever you use a method from rhinoscriptsyntax, always ensure that it is an official part of the module, and that you use the correct number and types of arguments for each function. Your final code output should precisely capture the user’s intent, be free of ambiguity, and run without errors.\\n\\nOutput should be a python code inside ```python ``` block. \"\"\"},\n",
        "    {\"role\" : \"user\",\n",
        "     \"content\" : \"how to create a box.\"}\n",
        "]\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking = True\n",
        ")\n",
        "\n",
        "outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=2048,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        min_p=0\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "Fcj-iy9ACw0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After SFT"
      ],
      "metadata": {
        "id": "IGqHRWDYC0ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use a pipeline for easy inference\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\" : \"system\",\n",
        "     \"content\" : \"\"\"You are an expert in Python and scripting using the rhinoscriptsyntax module. Your task is to understand user query and generate a script accordingly to model in rhino 3d.\n",
        "     Please reason step by step. Breakdown the user query into parts, based on which write an algorithm, then write the code. Do not prompt user for input in the code unless specified.\n",
        "     Your Output should be a python code inside ```python ``` block. \"\"\"},\n",
        "    {\"role\" : \"user\",\n",
        "     \"content\" : \"create boxes along circle.\"}\n",
        "]\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking = True\n",
        ")\n",
        "\n",
        "outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=2048,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        min_p=0\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "okQXpBvfJ4Dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}