{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dt-cs/Qwen_SFT/blob/main/Qwen3_8b_10k_with_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iH95w8FXt9_O"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install fsspec==2023.9.2\n",
        "    !pip install -U transformers trl\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip install wandb\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "#dataset = load_dataset(\"deebak14/rhinoscript_ft_data_04\", split = \"train\")\n",
        "\n",
        "train_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_04\", split = \"train\")\n",
        "eval_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_04_test\", split = \"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen3-8B\"            #\"Qwen/Qwen2.5-Coder-7B-Instruct\" #\"mistralai/Mistral-7B-Instruct-v0.3\" #\"google/gemma-3-12b-it\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Setup bitsandbytes 4-bit config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load the model in 4bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",         # auto-assigns model to GPU if available\n",
        "    torch_dtype=torch.float16, # or 'auto' if you want auto-detection\n",
        ")\n",
        "\n",
        "# Set max sequence length when tokenizing/generating, not in .from_pretrained\n",
        "max_seq_length = 4096\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "4a4d905a-a109-4e71-b865-550812c41492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 10,911,744 || all params: 8,201,647,104 || trainable%: 0.1330\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=4,                     # LoRA rank\n",
        "    lora_alpha=8,           # LoRA alpha (often 2x r)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",    # Important for language modeling\n",
        ")\n",
        "\n",
        "# Attach LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# (Optional) Print trainable parameters to confirm LoRA is applied\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/qwen_chat_template.jinja\") as f:\n",
        "    tokenizer.chat_template = f.read()"
      ],
      "metadata": {
        "id": "wKrIXeN3g6cP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import apply_chat_template\n",
        "\n",
        "# Pick a few examples from your dataset\n",
        "for i in range(3):\n",
        "    sample = train_dataset[i]\n",
        "    # Apply the chat template with tokenization and assistant mask\n",
        "    output = tokenizer.apply_chat_template(\n",
        "        sample['messages'],\n",
        "        tokenize=True,\n",
        "        return_assistant_tokens_mask=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(\"\".join(map(str, output[\"assistant_masks\"])))\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "OEJy1T5Xl_aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    dataset_text_field=\"messages\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.05,                # Increased for more stable start\n",
        "    num_train_epochs=10,\n",
        "    learning_rate= 2e-6,             #1e-5             # Lowered for better generalization\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.05,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=3407,\n",
        "    report_to=\"wandb\",\n",
        "    eval_strategy=\"epoch\",          # Evaluate and save every epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,             # Only keep last 3 checkpoints\n",
        "    max_grad_norm=1.0,              # Clip gradients\n",
        "    fp16=True,                      # Enable mixed-precision if available (optional, can remove if not supported)\n",
        "    push_to_hub=False,\n",
        "    neftune_noise_alpha=5,\n",
        "    assistant_only_loss=True,\n",
        "    chat_template_path=\"/content/qwen_chat_template.jinja\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the full input_ids and assistant_masks for a sample\n",
        "sample = trainer.train_dataset[100]\n",
        "input_ids = sample[\"input_ids\"]\n",
        "assistant_masks = sample[\"assistant_masks\"]\n",
        "\n",
        "# Extract token ids for assistant tokens only\n",
        "assistant_token_ids = [tid for tid, mask in zip(input_ids, assistant_masks) if mask == 1]\n",
        "\n",
        "# Now decode ONLY those assistant tokens\n",
        "assistant_text = tokenizer.decode(assistant_token_ids)\n",
        "print(\"Assistant text only:\\n\", assistant_text)\n"
      ],
      "metadata": {
        "id": "EuMl3P5OexGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "6cec6a15-e457-4ae5-f9f3-db75c2897820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "7.773 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "10c48aa3-f075-4a1f-e4d1-f91de3d15c6d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdtamilmani14\u001b[0m (\u001b[33mdtamilmani14-penn-state\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250713_105447-plcayrgr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dtamilmani14-penn-state/huggingface/runs/plcayrgr' target=\"_blank\">trainer_output</a></strong> to <a href='https://wandb.ai/dtamilmani14-penn-state/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dtamilmani14-penn-state/huggingface' target=\"_blank\">https://wandb.ai/dtamilmani14-penn-state/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dtamilmani14-penn-state/huggingface/runs/plcayrgr' target=\"_blank\">https://wandb.ai/dtamilmani14-penn-state/huggingface/runs/plcayrgr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9652' max='11580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9652/11580 6:06:21 < 1:13:11, 0.44 it/s, Epoch 8.33/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.590800</td>\n",
              "      <td>0.918912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.497200</td>\n",
              "      <td>0.843846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.463700</td>\n",
              "      <td>0.811876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.501300</td>\n",
              "      <td>0.794108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>0.782978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.444000</td>\n",
              "      <td>0.776075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.433200</td>\n",
              "      <td>0.772615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.408900</td>\n",
              "      <td>0.770515</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "cf2823c4-a07f-440e-aea9-60ac9fb32a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3875.2585 seconds used for training.\n",
            "64.59 minutes used for training.\n",
            "Peak reserved memory = 32.387 GB.\n",
            "Peak reserved memory for training = 18.532 GB.\n",
            "Peak reserved memory % of max memory = 81.874 %.\n",
            "Peak reserved memory for training % of max memory = 46.849 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
        "\n",
        "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before SFT"
      ],
      "metadata": {
        "id": "KwE7s5YtCyOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use a pipeline for easy inference\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\" : \"system\",\n",
        "     \"content\" : \"\"\"You are an expert in Python and Rhino 3D modeling using the rhinoscriptsyntax module and, if required, other native Python modules to accomplish the task. Your task is to interpret, analyze, and understand the user’s query, then respond with a corresponding Python script. Whenever you use a method from rhinoscriptsyntax, always ensure that it is an official part of the module, and that you use the correct number and types of arguments for each function. Your final code output should precisely capture the user’s intent, be free of ambiguity, and run without errors.\\n\\nOutput should be a python code inside ```python ``` block. \"\"\"},\n",
        "    {\"role\" : \"user\",\n",
        "     \"content\" : \"how to create a box.\"}\n",
        "]\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking = True\n",
        ")\n",
        "\n",
        "outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=2048,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        min_p=0\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "Fcj-iy9ACw0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After SFT"
      ],
      "metadata": {
        "id": "IGqHRWDYC0ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use a pipeline for easy inference\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\" : \"system\",\n",
        "     \"content\" : \"\"\"You are an expert in Python and scripting using the rhinoscriptsyntax module. Your task is to understand user query and generate a script accordingly to model in rhino 3d.\n",
        "     Please reason step by step. Breakdown the user query into parts, based on which write an algorithm, then write the code. Do not prompt user for input in the code unless specified.\n",
        "     Your Output should be a python code inside ```python ``` block. \"\"\"},\n",
        "    {\"role\" : \"user\",\n",
        "     \"content\" : \"create 8 cubes along the circle\"}\n",
        "]\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking = False\n",
        ")\n",
        "\n",
        "outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=4096,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        top_k=20,\n",
        "        min_p=0\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "okQXpBvfJ4Dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}