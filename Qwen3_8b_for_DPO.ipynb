{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dt-cs/Qwen_SFT/blob/main/Qwen3_8b_for_DPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH95w8FXt9_O"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers trl bitsandbytes peft accelerate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_05_train\", split = \"train\")\n",
        "eval_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_05_eval\", split = \"train\")\n",
        "#dpo_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_dpo_01\", split = \"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)\n",
        "print(eval_dataset)\n",
        "\n",
        "train_dataset[0]\n",
        "eval_dataset[0]"
      ],
      "metadata": {
        "id": "Icw2AXJoyGyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen3-8B\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Setup bitsandbytes 4-bit config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load the model in 4bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",         # auto-assigns model to GPU if available\n",
        "    torch_dtype=torch.float16, # or 'auto' if you want auto-detection\n",
        ")\n",
        "\n",
        "# Set max sequence length when tokenizing/generating, not in .from_pretrained\n",
        "max_seq_length = 3072\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "324b2ec7-ad80-49ef-a280-3055deae935c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 10,911,744 || all params: 8,201,647,104 || trainable%: 0.1330\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=4,                     # LoRA rank\n",
        "    lora_alpha=8,           # LoRA alpha (often 2x r)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",    # Important for language modeling\n",
        ")\n",
        "\n",
        "# Attach LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# (Optional) Print trainable parameters to confirm LoRA is applied\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "def evaluate_model_loss(model, tokenizer, dataset, max_length=2048):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for example in tqdm(dataset, desc=\"Evaluating\"):\n",
        "        messages = example[\"messages\"]\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            loss = outputs.loss.item()\n",
        "            losses.append(loss)\n",
        "    return sum(losses) / len(losses)"
      ],
      "metadata": {
        "id": "tq4gbJ0oKTXn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_loss = evaluate_model_loss(model, tokenizer, eval_dataset)\n",
        "perplexity = math.exp(mean_loss)\n",
        "\n",
        "print(f\"\\nâœ… Mean loss of Base model: {mean_loss:.4f}\")\n",
        "print(f\"ðŸ“‰ Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTcRU4DSKbLJ",
        "outputId": "011b38bf-a443-4892-e7d7-18c097ff00de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 476/476 [01:33<00:00,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Mean loss of Base model: 2.7162\n",
            "ðŸ“‰ Perplexity: 15.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/qwen_chat_template.jinja\") as f:\n",
        "    tokenizer.chat_template = f.read()"
      ],
      "metadata": {
        "id": "wKrIXeN3g6cP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import apply_chat_template\n",
        "\n",
        "# Pick a few examples from your dataset\n",
        "for i in range(3):\n",
        "    sample = train_dataset[i]\n",
        "    # Apply the chat template with tokenization and assistant mask\n",
        "    output = tokenizer.apply_chat_template(\n",
        "        sample['messages'],\n",
        "        tokenize=True,\n",
        "        return_assistant_tokens_mask=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(\"\".join(map(str, output[\"assistant_masks\"])))\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "OEJy1T5Xl_aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    dataset_text_field=\"messages\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.05,                # Increased for more stable start\n",
        "    num_train_epochs=10,\n",
        "    learning_rate= 2e-6,             #1e-5             # Lowered for better generalization\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.05,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=3407,\n",
        "    report_to=\"none\",\n",
        "    eval_strategy=\"epoch\",          # Evaluate and save every epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,             # Only keep last 3 checkpoints\n",
        "    max_grad_norm=1.0,              # Clip gradients\n",
        "    fp16=True,                      # Enable mixed-precision if available (optional, can remove if not supported)\n",
        "    push_to_hub=False,\n",
        "    neftune_noise_alpha=5,\n",
        "    assistant_only_loss=True,\n",
        "    chat_template_path=\"/content/qwen_chat_template.jinja\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the full input_ids and assistant_masks for a sample\n",
        "sample = trainer.train_dataset[100]\n",
        "input_ids = sample[\"input_ids\"]\n",
        "assistant_masks = sample[\"assistant_masks\"]\n",
        "\n",
        "# Extract token ids for assistant tokens only\n",
        "assistant_token_ids = [tid for tid, mask in zip(input_ids, assistant_masks) if mask == 1]\n",
        "\n",
        "# Now decode ONLY those assistant tokens\n",
        "assistant_text = tokenizer.decode(assistant_token_ids)\n",
        "print(\"Assistant text only:\\n\", assistant_text)\n"
      ],
      "metadata": {
        "id": "EuMl3P5OexGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "4ef6572a-ead5-4688-fb9f-62d949bcad2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "8.605 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "aff16cc8-6525-4859-f6ba-b4b2b07e899d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2980' max='2980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2980/2980 1:53:14, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.775000</td>\n",
              "      <td>1.731180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.159200</td>\n",
              "      <td>1.120746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.767100</td>\n",
              "      <td>0.770670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.710100</td>\n",
              "      <td>0.711665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.668400</td>\n",
              "      <td>0.676069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.630900</td>\n",
              "      <td>0.652364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.676600</td>\n",
              "      <td>0.637848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.626400</td>\n",
              "      <td>0.630210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.608300</td>\n",
              "      <td>0.627392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.599000</td>\n",
              "      <td>0.626988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_loss = evaluate_model_loss(model, tokenizer, eval_dataset)\n",
        "perplexity = math.exp(mean_loss)\n",
        "\n",
        "print(f\"\\nâœ… Mean loss of fine-tuned model: {mean_loss:.4f}\")\n",
        "print(f\"ðŸ“‰ Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrH6m_UpKiwB",
        "outputId": "71e33cc1-12dd-4442-de6f-d91e967e6ccf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 476/476 [01:36<00:00,  4.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Mean loss of fine-tuned model: 1.4342\n",
            "ðŸ“‰ Perplexity: 4.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"deebak14/qwen_8b_ft_v1\") # Online saving\n",
        "tokenizer.push_to_hub(\"deebak14/qwen_8b_ft_v1\") # Online saving"
      ],
      "metadata": {
        "id": "IFqkt2E3N-JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full dataset\n",
        "dpo_dataset = load_dataset(\"deebak14/rhinoscript_ft_data_dpo_01\", split=\"train\")\n",
        "\n",
        "# Split into 85% train and 15% eval\n",
        "split_dataset = dpo_dataset.train_test_split(test_size=0.15, seed=42)\n",
        "\n",
        "# Access the subsets\n",
        "dpo_train_dataset = split_dataset[\"train\"]\n",
        "dpo_eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "# Optional: Check sizes\n",
        "print(f\"Train size: {len(dpo_train_dataset)}\")\n",
        "print(f\"Eval size: {len(dpo_eval_dataset)}\")"
      ],
      "metadata": {
        "id": "znDO4mNzP0mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dpo_train_dataset)\n",
        "print(dpo_eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCorQ5tne6Y4",
        "outputId": "efdffdc7-7bef-411e-f9d0-89f242bb247a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['prompt', 'chosen', 'rejected'],\n",
            "    num_rows: 2427\n",
            "})\n",
            "Dataset({\n",
            "    features: ['prompt', 'chosen', 'rejected'],\n",
            "    num_rows: 429\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOConfig, DPOTrainer\n",
        "\n",
        "dpo_config = DPOConfig(\n",
        "    beta=0.1,\n",
        "    max_prompt_length=512,\n",
        "    max_length=2048,\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_ratio = 0.1,\n",
        "    num_train_epochs = 3,\n",
        "    learning_rate= 2e-6,\n",
        "    fp16 = True,\n",
        "    logging_steps = 10,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,             # Only keep last 3 checkpoints\n",
        "    max_grad_norm=1.0,\n",
        "    output_dir = \"/content/outputs\",\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,  # Uses frozen copy of base model internally\n",
        "    args=dpo_config,\n",
        "    train_dataset=dpo_train_dataset,\n",
        "    eval_dataset=None,\n",
        ")"
      ],
      "metadata": {
        "id": "Apz_3yjbfiKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer_stats = dpo_trainer.train()"
      ],
      "metadata": {
        "id": "rPVLYSHGhqZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
        "\n",
        "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After SFT"
      ],
      "metadata": {
        "id": "IGqHRWDYC0ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use a pipeline for easy inference\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\" : \"system\",\n",
        "     \"content\" : \"\"\"You are an expert in Python and scripting using the rhinoscriptsyntax module. Your task is to understand user query and generate a script accordingly to model in rhino 3d.\n",
        "     Please reason step by step. Breakdown the user query into parts, based on which write an algorithm, then write the code. Do not prompt user for input in the code unless specified.\n",
        "     Your Output should be a python code inside ```python ``` block. \"\"\"},\n",
        "    {\"role\" : \"user\",\n",
        "     \"content\" : \"create 8 cubes along the circle\"}\n",
        "]\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking = True\n",
        ")\n",
        "\n",
        "outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=4096,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        top_k=20,\n",
        "        min_p=0\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "okQXpBvfJ4Dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69fcfc7-52bc-414b-f0ad-1b94e149b462"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "You are an expert in Python and scripting using the rhinoscriptsyntax module. Your task is to understand user query and generate a script accordingly to model in rhino 3d.\n",
            "     Please reason step by step. Breakdown the user query into parts, based on which write an algorithm, then write the code. Do not prompt user for input in the code unless specified.\n",
            "     Your Output should be a python code inside ```python ``` block. <|im_end|>\n",
            "<|im_start|>user\n",
            "create 8 cubes along the circle<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "Let's break down the user query and create a script that creates 8 cubes along a circle in Rhino 3D.\n",
            "\n",
            "1. First, we need to create a circle in Rhino 3D. The circle can be defined by a center point, a radius, and a plane.\n",
            "2. Next, we need to divide the circle into 8 equal segments. This can be done by creating points along the circle's circumference.\n",
            "3. Then, we need to create cubes at each of these points. Each cube should be aligned with the circle's plane and have a specific size.\n",
            "4. Finally, we need to add the cubes to the model.\n",
            "\n",
            "The code below creates a circle with a center point at the origin, a radius of 10 units, and a plane aligned with the world XY plane. It then divides the circle into 8 equal segments and creates cubes at each of these points. Each cube is aligned with the circle's plane and has a size of 2 units.\n",
            "\n",
            "The code also includes comments to explain each step of the process.\n",
            "</think>\n",
            "\n",
            "```python\n",
            "import rhinoscriptsyntax as rs\n",
            "\n",
            "# Define the circle parameters\n",
            "center = (0, 0, 0)\n",
            "radius = 10\n",
            "plane = rs.WorldXYPlane()\n",
            "\n",
            "# Create the circle\n",
            "circle = rs.AddCircle(center, radius, plane)\n",
            "\n",
            "# Divide the circle into 8 equal segments\n",
            "points = rs.DivideCircle(circle, 8)\n",
            "\n",
            "# Create cubes at each point\n",
            "for point in points:\n",
            "    # Define the cube size\n",
            "    cube_size = 2\n",
            "    \n",
            "    # Create the cube\n",
            "    cube = rs.AddBox(rs.BoundingBox(point, (cube_size, cube_size, cube_size)))\n",
            "    \n",
            "    # Move the cube to the point\n",
            "    rs.MoveObject(cube, point)\n",
            "```\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}